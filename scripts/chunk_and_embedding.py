# run_ingest.py
# (Giai Ä‘oáº¡n 2: Äá»c JSON -> Chunk -> Embed -> Náº¡p vÃ o Vector Store)

import os
import sys
import json
import re
from pathlib import Path

from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter, Language
from langchain_core.documents import Document

# --- Setup Paths ---
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.append(str(PROJECT_ROOT))

from config import JSON_OUTPUT_DIR, VECTOR_STORE_DIR, CHUNK_SIZE, CHUNK_OVERLAP
from src.chatbot.core.utils import get_embedding_model

# --- LOGIC CHUNKING (Chuyá»ƒn tá»« file cÅ© sang) ---

def chunk_unstructured_elements(elements: list[dict]) -> list[Document]:
    """
    Chunk 1 list cÃ¡c 'elements' (dáº¡ng dict) tá»« file JSON.
    ÄÃ¢y lÃ  logic 'chunk_elements' cÅ© cá»§a anh, nhÆ°ng Ä‘á»c tá»« dict.
    """
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
    )
    chunks = []
    current_text_batch = ""

    for el in elements:
        el_type = el.get("type", "").lower()
        text = el.get("text", "")
        
        if el_type == "table":
            if current_text_batch.strip():
                chunks.extend(text_splitter.create_documents([current_text_batch]))
                current_text_batch = ""
            
            # Xá»­ lÃ½ báº£ng
            table_text = el.get("text")
            chunks.append(Document(page_content=f"ÄÃ¢y lÃ  má»™t báº£ng: {table_text}", metadata={"type": "table"}))
        
        elif el_type in ("narrativetext", "listitem", "title"):
            current_text_batch += text + "\n\n"
    
    if current_text_batch.strip():
        chunks.extend(text_splitter.create_documents([current_text_batch]))
    
    return chunks

def chunk_table_rows(rows: list[dict]) -> list[Document]:
    """
    Chunk dá»¯ liá»‡u dáº¡ng hÃ ng (tá»« CSV/DAT). Má»—i hÃ ng lÃ  1 Document.
    ÄÃ¢y lÃ  logic 'process_csv_file' cÅ© cá»§a anh.
    """
    chunks = []
    for i, row_dict in enumerate(rows):
        content_parts = [f"{str(col).strip()}: {str(val).strip()}" for col, val in row_dict.items()]
        page_content = ", ".join(content_parts)
        metadata = {"type": "csv_row", "row_index": i + 1}
        chunks.append(Document(page_content=page_content, metadata=metadata))
    return chunks

def chunk_plain_text(content: str) -> list[Document]:
    """Chunk file text Ä‘Æ¡n giáº£n (tá»« .txt)"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
    )
    return text_splitter.create_documents([content])

def chunk_code(content: dict) -> list[Document]:
    """Chunk file code (logic tá»« 'process_code_file' cÅ©)"""
    lang = content.get("language", "text")
    text = content.get("content", "")
    
    try:
        lang_enum = Language(lang) # Cá»‘ gáº¯ng map sang enum
        text_splitter = RecursiveCharacterTextSplitter.from_language(
            language=lang_enum, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
        )
    except: # Náº¿u tháº¥t báº¡i (vÃ­ dá»¥ lang="text"), dÃ¹ng splitter máº·c Ä‘á»‹nh
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
        )
        
    return text_splitter.create_documents([text])

# --- HÃ€M MAIN Cá»¦A GIAI ÄOáº N 2 ---

def main():
    print("--- Báº®T Äáº¦U GIAI ÄOáº N 2: INGEST JSON VÃ€O VECTOR STORE ---")
    
    if not JSON_OUTPUT_DIR.exists():
        print(f"ThÆ° má»¥c JSON {JSON_OUTPUT_DIR} khÃ´ng tá»“n táº¡i. HÃ£y cháº¡y run_parser.py trÆ°á»›c.")
        return

    print("Äang táº£i model embedding...")
    embeddings = get_embedding_model()
    
    print("Khá»Ÿi táº¡o Chroma Vector Store...")
    store_path = VECTOR_STORE_DIR / "global"
    store_path.mkdir(parents=True, exist_ok=True)
    vectorstore = Chroma(
        persist_directory=str(store_path),
        embedding_function=embeddings
    )

    # Láº¥y metadata cÅ© Ä‘á»ƒ check trÃ¹ng láº·p
    existing_metadatas = []
    try:
        existing_metadatas = vectorstore.get()["metadatas"]
    except Exception:
        pass

    def is_duplicate(topic, doc_id):
        return any(
            meta.get("topic") == topic and meta.get("document_id") == doc_id
            for meta in existing_metadatas
        )

    print(f"QuÃ©t thÆ° má»¥c JSON: {JSON_OUTPUT_DIR}")
    
    for json_path in JSON_OUTPUT_DIR.glob("**/*.json"):
        if not json_path.is_file():
            continue
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            source_filename = data.get("source_filename", "unknown")
            data_type = data.get("data_type", "unknown")
            content = data.get("content")

            # --- ThÃ´ng tin metadata tá»« Ä‘Æ°á»ng dáº«n file JSON ---
            relative_path = json_path.relative_to(JSON_OUTPUT_DIR)
            topic = str(relative_path.parent)
            if topic == ".": topic = "general"
            # Láº¥y tÃªn file gá»‘c (BaoCao.pdf -> BaoCao)
            document_id = relative_path.name.split('.')[0] 

            print(f"Processing JSON: {json_path.name} (topic: {topic}, id: {document_id})")

            # --- CHECK TRÃ™NG Láº¶P (Logic cÅ© cá»§a anh) ---
            if is_duplicate(topic, document_id):
                print(f"ğŸ§¹ XoÃ¡ dá»¯ liá»‡u cÅ©: topic='{topic}', id='{document_id}'")
                vectorstore._collection.delete(where={"topic": topic, "document_id": document_id})

            if not content:
                print("  Bá» qua (khÃ´ng cÃ³ content).")
                continue

            # --- CHUNKING ROUTER (Dá»±a trÃªn data_type) ---
            splits = []
            if data_type == "unstructured_doc":
                splits = chunk_unstructured_elements(content)
            elif data_type == "table_rows":
                splits = chunk_table_rows(content)
            elif data_type == "plain_text":
                splits = chunk_plain_text(content.get("content", ""))
            elif data_type == "code":
                splits = chunk_code(content)
            
            if not splits:
                print("  KhÃ´ng táº¡o Ä‘Æ°á»£c chunk nÃ o.")
                continue

            # GÃ¡n metadata chung cho táº¥t cáº£ chunks
            for chunk in splits:
                chunk_metadata = chunk.metadata if chunk.metadata is not None else {}
                chunk_metadata.update({
                    "topic": topic, 
                    "document_id": document_id, 
                    "source": source_filename
                })
                chunk.metadata = chunk_metadata

            # ThÃªm vÃ o vector store
            vectorstore.add_documents(splits)
            print(f"  -> Cáº­p nháº­t xong: {len(splits)} chunks.")

        except Exception as e:
            print(f"  Lá»–I khi xá»­ lÃ½ JSON {json_path.name}: {e}")

    vectorstore.persist()
    print(f"\nÄÃ£ lÆ°u vectorstore tá»•ng há»£p táº¡i: {store_path}")
    print("\nğŸ‰ HOÃ€N Táº¤T GIAI ÄOáº N 2: INGEST VECTOR STORE")

if __name__ == "__main__":
    main()